{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importations\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function definition\n",
    "def clean_tweet(tweet):#function used to clean a single tweet\n",
    "    tweet = str(tweet)\n",
    "    tweet = re.sub(\"(http:\\/\\/)[^ ]*\",\"\",tweet)\n",
    "    tweet = tweet.replace(\"&amp;\",\"\")\n",
    "    tweet = re.sub(\"[^\\w\\s]\",\"\",tweet)\n",
    "    tweet = re.sub(\"[\\d]\",\"\",tweet)\n",
    "    tweet = tweet.lower()\n",
    "    return tweet\n",
    "def remove_rare_words(tweet,rar):#function used to remove the common words (in com) and rare words (in rar) from a single tweet\n",
    "    tweetWords = tweet.split(\" \")\n",
    "    newTweet = \"\"\n",
    "    for word in tweetWords:\n",
    "        if (word not in rar):\n",
    "            newTweet += word + \" \"\n",
    "    return newTweet\n",
    "def remove_common_words(tweet,com):#function used to remove the common words (in com) and rare words (in rar) from a single tweet\n",
    "    tweetWords = tweet.split(\" \")\n",
    "    newTweet = \"\"\n",
    "    for word in tweetWords:\n",
    "        if (word not in com):\n",
    "            newTweet += word + \" \"\n",
    "    return newTweet\n",
    "def remove_single_letter_words(tweet):#function used to remove single letter words ('m' 'n', 'I'...)\n",
    "    tweetWords = tweet.split(\" \")\n",
    "    newTweet = \"\"\n",
    "    for word in tweetWords:\n",
    "        if len(word) != 1:\n",
    "            newTweet += word + \" \"\n",
    "    return newTweet\n",
    "def clean_dataset(dataset):#function used to clean an entire dataset, uses the three previously declared functions\n",
    "    dataset = dataset.apply(lambda x: clean_tweet(x))\n",
    "    frequencyOfWords = pd.Series(' '.join(dataset).split()).value_counts()\n",
    "    #commonWords = frequencyOfWords[:10] # 10 most common words\n",
    "    rareWords = frequencyOfWords[-10:] # 10 rarest words\n",
    "    dataset = dataset.apply(lambda x: remove_rare_words(x,rareWords))\n",
    "    #dataset = dataset.apply(lambda x: remove_rare_words(x,commonWords))\n",
    "    dataset = dataset.apply(lambda x: remove_single_letter_words(x))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training set and testing set\n",
    "train_data = pd.read_csv('train_preprocessed.csv')\n",
    "test_data = pd.read_csv('test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning both training and testing sets\n",
    "train_data['comment_text'] = clean_dataset(train_data['comment_text'])\n",
    "test_data['comment_text'] = clean_dataset(test_data['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply features extraction algorithm to train set\n",
    "count_vect = CountVectorizer(ngram_range=(1,2), min_df=3, max_df=0.9)\n",
    "train_data_counts = count_vect.fit_transform(train_data['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply TF-IDF to training set\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=1, sublinear_tf=1)\n",
    "train_data_tfidf = tfidf_transformer.fit_transform(train_data_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed the cleaned tweets and their classification to the classifier\n",
    "#print(train_data_tfidf.shape)\n",
    "#print(train_data['toxic'].shape)\n",
    "#Y = train_data['insult'] + train_data['threat'] + train_data['toxic']\n",
    "Y = train_data['toxic']\n",
    "clf = MultinomialNB().fit(train_data_tfidf, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply features extraction algorithm and TF-IDF to the testing set\n",
    "test_data_counts = count_vect.transform(test_data['comment_text'])\n",
    "test_data_tfidf = tfidf_transformer.transform(test_data_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the classification of the testing set\n",
    "predicted = clf.predict(test_data_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'yo bitch ja rule is more succesful then you ll ever be whats up with you and hating you sad mo  fuck  as should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me  ja rule is about pride in da music man  dont diss that  shit  on him  and nothin is wrong bein like tupac he was brother too fuck  in white boys get things right next time    ' => 0.0\n"
     ]
    }
   ],
   "source": [
    "#Display both tweets from the test set and their predicted classification\n",
    "for tweet, category in zip(test_data['comment_text'], predicted):\n",
    "        print('%r => %s' % (tweet, category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a csv file to store the submission\n",
    "header = [\"comment_text\",\"toxic\"]\n",
    "rows = zip(test_data['comment_text'],predicted)\n",
    "with open('sample_submission.csv', 'w') as submission:\n",
    "    wr = csv.writer(submission, delimiter=',',lineterminator='\\n', quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(header)\n",
    "    for row in rows:\n",
    "        wr.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finalized_tfidftransformer.sav']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "# now you can save it to a file\n",
    "#joblib.dump(clf, 'filename.pkl') \n",
    "# and later you can load it\n",
    "#clf = joblib.load('filename.pkl')\n",
    "\n",
    "filename = 'finalized_model.sav'\n",
    "joblib.dump(clf, filename)\n",
    "\n",
    "filename = 'finalized_countvectorizer.sav'\n",
    "joblib.dump(count_vect, filename)\n",
    "\n",
    "filename = 'finalized_tfidftransformer.sav'\n",
    "joblib.dump(tfidf_transformer, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "test_data_counts = count_vect.transform([clean_tweet(\"adorable\")])\n",
    "test_data_tfidf = tfidf_transformer.transform(test_data_counts)\n",
    "predicted = clf.predict(test_data_tfidf)\n",
    "print(predicted)\n",
    "\n",
    "#use comment model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
